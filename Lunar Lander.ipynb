{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef3f9b70-bda2-4ccd-8c9c-dfeae8f949dd",
   "metadata": {},
   "source": [
    "# 0. import dependcies of what we will be using \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d3161c-b597-414d-a15a-0b259085ca79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym \n",
    "from stable_baselines3 import DQN \n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.evaluation import evaluate_policy  \n",
    "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold \n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "import os \n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be36c0ea-e0e0-41a5-8ece-3d93bfa9a5bf",
   "metadata": {},
   "source": [
    "# 1. Making The Enviornment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ef5eed5-a8de-4903-bea3-2a6c4077fdbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"LunarLander-v2\",render_mode = 'human') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a5fc3e-0f25-4bff-a7ab-036b9eb6d549",
   "metadata": {},
   "source": [
    "# 2. Designating a Path "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5add6bcf-dfe8-4b34-ba4a-34ad30ac0935",
   "metadata": {},
   "outputs": [],
   "source": [
    "DQN_path = os.path.join('RL','Model')  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e382ffdb-6ba8-4606-b740-14aabeca9636",
   "metadata": {},
   "source": [
    "# 3. Callbacks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a89d6d7c-5bce-42ad-bf24-9e548478fd6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "stoptraining = StopTrainingOnRewardThreshold(reward_threshold=260,verbose=1) \n",
    "evalcallback = EvalCallback(env,callback_on_new_best = stoptraining,verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422da266-f379-4fef-89d8-7e05c7c177e1",
   "metadata": {},
   "source": [
    "# 4. Applying Wrappers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8125527a-94f8-4a0f-b17a-706daa10311f",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make_vec_env('LunarLander-v2', n_envs=3, seed=None, start_index=0, \n",
    "                   monitor_dir=None, wrapper_class=None, env_kwargs=None, \n",
    "                   vec_env_cls=None, vec_env_kwargs=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aeba82b-bdd4-44eb-849c-b6d2ffcf68b8",
   "metadata": {},
   "source": [
    "# 5. Training Environment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9daf0bcd-1bb2-4407-af7e-8f8c1cb238a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model =DQN(\"MlpPolicy\",env, learning_rate=0.0001, buffer_size=1000000, \n",
    "           learning_starts=50000, batch_size=32, tau=1.0, gamma=0.99, \n",
    "           train_freq=4, gradient_steps=1, replay_buffer_class=None, \n",
    "           replay_buffer_kwargs=None, optimize_memory_usage=False, \n",
    "           target_update_interval=10000, exploration_fraction=0.1, \n",
    "           exploration_initial_eps=1.0, exploration_final_eps=0.05, \n",
    "           max_grad_norm=10, stats_window_size=100, tensorboard_log=None, \n",
    "           policy_kwargs=None, verbose=1, seed=3, device='cuda', \n",
    "           _init_setup_model=True)\n",
    "model.learn(total_timesteps=4000000, callback = evalcallback) \n",
    "model.save(DQN_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c883454e-2a54-4fcf-af0f-a24dcc1a6548",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7f049c5e-f7d5-471d-b339-b953d4668357",
   "metadata": {},
   "source": [
    "# 6.loading Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3f6595be-0184-40d5-9af4-caf2e1383e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(DQN_path)\n",
    "model = DQN.load(DQN_path) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68ccfc8-f1b5-4c3c-8500-75897830e711",
   "metadata": {},
   "source": [
    "# 7. Evalutating Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a79501c9-6c4c-4f56-a250-aee9dae3ad4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(204.75261719999997, 52.779680973137474)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_policy(model, env,n_eval_episodes = 10, render = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff456c19-f8ad-44b1-a396-599cd767ce95",
   "metadata": {},
   "source": [
    "# 8. Test out Algorithmn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6789904b-6179-4be4-8bc3-d18c507a6f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()\n",
    "while True:\n",
    "    action, states = model.predict(obs, deterministic=True)\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    if terminated or truncated:\n",
    "        obs, info = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111181c9-184b-4292-b7dd-51b2ea171c61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
